{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            author : a life in photography -- in pictures\n",
      "1                  an angler fishes river on a snowy day .\n",
      "2        photograph of the sign being repaired by brave...\n",
      "3        the player staring intently at a computer scre...\n",
      "4        globes : the green 3d person carrying in hands...\n",
      "                               ...                        \n",
      "15835    a bougainvillea with pink flowers on a white b...\n",
      "15836        ingredient hanging over river during festival\n",
      "15837            the general circulation of the atmosphere\n",
      "15838    young teenager and her black horse in a traini...\n",
      "15839    person warms up during a game against american...\n",
      "Name: 0, Length: 15840, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your TSV file\n",
    "train_tsv_file_path = '/tmp/cyc/23Fall-269/Train_GCC-training.tsv'\n",
    "val_tsv_file_path = '/tmp/cyc/23Fall-269/Validation_GCC-1.1.0-Validation.tsv'\n",
    "\n",
    "# Read the TSV file into a DataFrame\n",
    "train_df = pd.read_csv(train_tsv_file_path, delimiter='\\t', header=None)[0]\n",
    "val_df = pd.read_csv(val_tsv_file_path, delimiter='\\t', header=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/miniconda3/envs/NLG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 18.2MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.01MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 1.67MB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sentence = self.data.iloc[idx]\n",
    "        \n",
    "        # Tokenize and encode the source sentence\n",
    "        source_tokens = self.tokenizer.encode_plus(\n",
    "            source_sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': source_tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': source_tokens['attention_mask'].squeeze(),\n",
    "            'target_ids': source_tokens['input_ids'].squeeze(),  # Target is the same as the input\n",
    "            'target_mask': source_tokens['attention_mask'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, model_name='t5-small'):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder_decoder = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, target_ids=None, target_mask=None):\n",
    "        if target_ids is not None:\n",
    "            # Training mode: input and target are provided\n",
    "            outputs = self.encoder_decoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=target_ids\n",
    "            )\n",
    "            return outputs.loss\n",
    "        else:\n",
    "            # Inference mode: only input is provided\n",
    "            outputs = self.encoder_decoder.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=50,  # Set a reasonable maximum length for generated sequences\n",
    "                num_beams=1,  # Set to 1 for greedy decoding\n",
    "                no_repeat_ngram_size=2,  # Avoid repeating bigrams in the output\n",
    "                early_stopping=True\n",
    "            )\n",
    "            return outputs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Create the dataset and DataLoader\n",
    "train_dataset = Seq2SeqDataset(train_df, tokenizer)\n",
    "val_dataset = Seq2SeqDataset(train_df, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "autoencoder_model = Autoencoder()\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = optim.AdamW(autoencoder_model.parameters(), lr=5e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    autoencoder_model.train()\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target_ids = batch['target_ids']\n",
    "        target_mask = batch['target_mask']\n",
    "\n",
    "        loss = autoencoder_model(input_ids, attention_mask, target_ids, target_mask)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}')\n",
    "\n",
    "    # Optionally update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate with Exact Match (EM) on a validation set\n",
    "    autoencoder_model.eval()\n",
    "    with torch.no_grad():\n",
    "        em_count = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for val_batch in tqdm(val_dataloader, desc=f'Validation - Epoch {epoch + 1}'):\n",
    "            input_ids = val_batch['input_ids']\n",
    "            attention_mask = val_batch['attention_mask']\n",
    "            target_ids = val_batch['target_ids']\n",
    "            target_mask = val_batch['target_mask']\n",
    "\n",
    "            # Generate sequences\n",
    "            generated_ids = autoencoder_model(input_ids, attention_mask).cpu().numpy()\n",
    "\n",
    "            # Decode token IDs to strings\n",
    "            generated_sentences = [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "            target_sentences = val_batch['target']\n",
    "\n",
    "            # Check for exact match\n",
    "            em_count += sum(1 for gen, target in zip(generated_sentences, target_sentences) if gen == target)\n",
    "            total_samples += len(generated_sentences)\n",
    "\n",
    "        em_score = em_count / total_samples\n",
    "        print(f'Validation EM Score: {em_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m608.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
