{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Specify the path to your TSV file\n",
    "train_tsv_file_path = '/home/allenfu/cyc/23Fall-269/Train_GCC-training.tsv'\n",
    "val_tsv_file_path = '/home/allenfu/cyc/23Fall-269/Validation_GCC-1.1.0-Validation.tsv'\n",
    "\n",
    "# Read the TSV file into a DataFrame\n",
    "train_df = pd.read_csv(train_tsv_file_path, delimiter='\\t', header=None)[0]\n",
    "val_df = pd.read_csv(val_tsv_file_path, delimiter='\\t', header=None)[0]\n",
    "\n",
    "def remove_spaces(sentence):\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(f' {punctuation}', punctuation)\n",
    "    return ' '.join(sentence.split())\n",
    "\n",
    "train_df = train_df.apply(remove_spaces)\n",
    "val_df = val_df.apply(remove_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=64):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sentence = self.data.iloc[idx]\n",
    "        \n",
    "        # Tokenize and encode the source sentence\n",
    "        source_tokens = self.tokenizer.encode_plus(\n",
    "            source_sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': source_tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': source_tokens['attention_mask'].squeeze(),\n",
    "            'target_ids': source_tokens['input_ids'].squeeze(),  # Target is the same as the input\n",
    "            'target_mask': source_tokens['attention_mask'].squeeze(),\n",
    "            'target': source_sentence\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allenfu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:199: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 51849/51849 [3:02:02<00:00,  4.75it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 0.005091056606930831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation - Epoch 1: 100%|██████████| 248/248 [01:59<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation EM Score: 0.8863636363636364\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:   2%|▏         | 907/51849 [03:10<2:58:37,  4.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     45\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     47\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda'\n",
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Create the dataset and DataLoader\n",
    "train_dataset = Seq2SeqDataset(train_df, tokenizer)\n",
    "val_dataset = Seq2SeqDataset(val_df, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = optim.AdamW(t5_model.parameters(), lr=5e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "best_em_score = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    t5_model.train()\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        target_mask = batch['target_mask'].to(device)\n",
    "\n",
    "        # Training mode: input and target are provided\n",
    "        outputs = t5_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=target_ids\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}')\n",
    "\n",
    "    # Optionally update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate with Exact Match (EM) on a validation set\n",
    "    t5_model.eval()\n",
    "    with torch.no_grad():\n",
    "        em_count = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for val_batch in tqdm(val_dataloader, desc=f'Validation - Epoch {epoch + 1}'):\n",
    "            input_ids = val_batch['input_ids'].to(device)\n",
    "            attention_mask = val_batch['attention_mask'].to(device)\n",
    "            target_ids = val_batch['target_ids'].to(device)\n",
    "            target_mask = val_batch['target_mask'].to(device)\n",
    "\n",
    "            # Inference mode: only input is provided\n",
    "            outputs = t5_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=64,  # Set a reasonable maximum length for generated sequences\n",
    "                num_beams=1,  # Set to 1 for greedy decoding\n",
    "                no_repeat_ngram_size=2,  # Avoid repeating bigrams in the output\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            # Generate sequences\n",
    "            generated_ids = outputs.detach().cpu().numpy()\n",
    "\n",
    "            # Decode token IDs to strings\n",
    "            generated_sentences = [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "            target_sentences = val_batch['target']\n",
    "\n",
    "            # Check for exact match\n",
    "            em_count += sum(1 for gen, target in zip(generated_sentences, target_sentences) if gen == target)\n",
    "            total_samples += len(generated_sentences)\n",
    "\n",
    "        em_score = em_count / total_samples\n",
    "        print(f'Validation EM Score: {em_score}')\n",
    "\n",
    "        # Save the model if the EM score improves\n",
    "        if em_score > best_em_score:\n",
    "            best_em_score = em_score\n",
    "            torch.save(t5_model.state_dict(), 't5_model.pth')\n",
    "            print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m608.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099,     0,  ..., 32099,     0, 32099],\n",
       "        [    0, 32099,     0,  ..., 32099,     0, 32099],\n",
       "        [    0, 32099,     0,  ..., 32099,     0, 32099]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    CLIPProcessor,\n",
    "    CLIPTextModel,\n",
    ")\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"sonoisa/t5-base-japanese\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"sonoisa/t5-base-japanese\", is_fast=True)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "tokenized_inputs = tokenizer(\n",
    "    [\"今日は良い天気です\", \"今日は良い天気です\", \"今良天気です今日今日今日\"],\n",
    "    add_special_tokens=True,\n",
    "    max_length=64,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ") # It's sunny today\n",
    "\n",
    "clip_inputs = processor(\n",
    "    text=[\"今日は良い天気です\", \"今日は良い天気です\", \"今良天気です今日今日今日\"],\n",
    "    images=torch.zeros(3, 3, 224, 224),\n",
    "    add_special_tokens=True,\n",
    "    max_length=64,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "clip_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "inputs_embeds = model.get_input_embeddings()(tokenized_inputs[\"input_ids\"])\n",
    "print(inputs_embeds.shape)\n",
    "\n",
    "# **NOTE**: pad_token_id is used as decoder_start_token_id\n",
    "dummy_decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]] * 3) \n",
    "\n",
    "output_ids = model.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    # attention_mask=tokenized_inputs[\"attention_mask\"],\n",
    "    decoder_input_ids=dummy_decoder_input_ids\n",
    ")\n",
    "\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: pillow\n",
      "Successfully installed pillow-10.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
