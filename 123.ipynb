{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cychang/anaconda3/envs/NLG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args: {'epoch': 50, 'lr': 0.0002, 'accum_iter': 2048, 'batch_size': 32, 'num_train_data': 200000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 103/6250 [01:22<1:21:54,  1.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 170\u001b[0m\n\u001b[1;32m    160\u001b[0m val_dataset \u001b[39m=\u001b[39m CLIPDataset(eval_images, processor, TRAIN_ROOT)\n\u001b[1;32m    163\u001b[0m train_args \u001b[39m=\u001b[39m {\n\u001b[1;32m    164\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m50\u001b[39m,\n\u001b[1;32m    165\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.0002\u001b[39m,\n\u001b[1;32m    166\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39maccum_iter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2048\u001b[39m,\n\u001b[1;32m    167\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m32\u001b[39m,\n\u001b[1;32m    168\u001b[0m }\n\u001b[0;32m--> 170\u001b[0m Trainer(\n\u001b[1;32m    171\u001b[0m   train_args \u001b[39m=\u001b[39;49m train_args,\n\u001b[1;32m    172\u001b[0m   model \u001b[39m=\u001b[39;49m model,\n\u001b[1;32m    173\u001b[0m   train_dataset \u001b[39m=\u001b[39;49m train_dataset,\n\u001b[1;32m    174\u001b[0m   val_dataset \u001b[39m=\u001b[39;49m val_dataset\n\u001b[1;32m    175\u001b[0m )\n",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m, in \u001b[0;36mTrainer\u001b[0;34m(train_args, model, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(train_args[\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m     69\u001b[0m   model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 70\u001b[0m   \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_dataloader)):\n\u001b[1;32m     71\u001b[0m       inputs \u001b[39m=\u001b[39m batch\n\u001b[1;32m     72\u001b[0m       inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m, in \u001b[0;36mCLIPDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mexcept\u001b[39;00m: \n\u001b[1;32m     35\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m inputs \u001b[39m=\u001b[39m processor(text\u001b[39m=\u001b[39;49mtext, images\u001b[39m=\u001b[39;49mimage, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, max_length \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m, truncation \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     38\u001b[0m inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m inputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/transformers/models/clip/processing_clip.py:103\u001b[0m, in \u001b[0;36mCLIPProcessor.__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(text, return_tensors\u001b[39m=\u001b[39mreturn_tensors, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m images \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_processor(images, return_tensors\u001b[39m=\u001b[39;49mreturn_tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m images \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     encoding[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m image_features\u001b[39m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/transformers/image_processing_utils.py:546\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    545\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/transformers/models/clip/image_processing_clip.py:303\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     images \u001b[39m=\u001b[39m [\n\u001b[1;32m    298\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor, input_data_format\u001b[39m=\u001b[39minput_data_format)\n\u001b[1;32m    299\u001b[0m         \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    300\u001b[0m     ]\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 303\u001b[0m     images \u001b[39m=\u001b[39m [\n\u001b[1;32m    304\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(image\u001b[39m=\u001b[39mimage, mean\u001b[39m=\u001b[39mimage_mean, std\u001b[39m=\u001b[39mimage_std, input_data_format\u001b[39m=\u001b[39minput_data_format)\n\u001b[1;32m    305\u001b[0m         \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    306\u001b[0m     ]\n\u001b[1;32m    308\u001b[0m images \u001b[39m=\u001b[39m [\n\u001b[1;32m    309\u001b[0m     to_channel_dimension_format(image, data_format, input_channel_dim\u001b[39m=\u001b[39minput_data_format) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    310\u001b[0m ]\n\u001b[1;32m    312\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/transformers/models/clip/image_processing_clip.py:304\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    297\u001b[0m     images \u001b[39m=\u001b[39m [\n\u001b[1;32m    298\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor, input_data_format\u001b[39m=\u001b[39minput_data_format)\n\u001b[1;32m    299\u001b[0m         \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    300\u001b[0m     ]\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m do_normalize:\n\u001b[1;32m    303\u001b[0m     images \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize(image\u001b[39m=\u001b[39;49mimage, mean\u001b[39m=\u001b[39;49mimage_mean, std\u001b[39m=\u001b[39;49mimage_std, input_data_format\u001b[39m=\u001b[39;49minput_data_format)\n\u001b[1;32m    305\u001b[0m         \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    306\u001b[0m     ]\n\u001b[1;32m    308\u001b[0m images \u001b[39m=\u001b[39m [\n\u001b[1;32m    309\u001b[0m     to_channel_dimension_format(image, data_format, input_channel_dim\u001b[39m=\u001b[39minput_data_format) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    310\u001b[0m ]\n\u001b[1;32m    312\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/transformers/image_processing_utils.py:616\u001b[0m, in \u001b[0;36mBaseImageProcessor.normalize\u001b[0;34m(self, image, mean, std, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    585\u001b[0m     image: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    591\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    592\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[39m    Normalize an image. image = (image - image_mean) / image_std.\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39m        `np.ndarray`: The normalized image.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     \u001b[39mreturn\u001b[39;00m normalize(\n\u001b[1;32m    617\u001b[0m         image, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, data_format\u001b[39m=\u001b[39;49mdata_format, input_data_format\u001b[39m=\u001b[39;49minput_data_format, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    618\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/transformers/image_transforms.py:398\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(image, mean, std, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    396\u001b[0m     image \u001b[39m=\u001b[39m ((image\u001b[39m.\u001b[39mT \u001b[39m-\u001b[39m mean) \u001b[39m/\u001b[39m std)\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 398\u001b[0m image \u001b[39m=\u001b[39m to_channel_dimension_format(image, data_format, input_data_format) \u001b[39mif\u001b[39;00m data_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m image\n\u001b[1;32m    399\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import requests\n",
    "import sys\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "\n",
    "class CLIPDataset(Dataset):\n",
    "  def __init__(self, data_path, processor, root):\n",
    "\n",
    "    self.dataset = data_path\n",
    "    self.processor = processor\n",
    "    self.root = root\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    text = self.dataset[idx].split(\".jpg\")[-2]\n",
    "\n",
    "    try:\n",
    "      image = Image.open(os.path.join(self.root, self.dataset[idx]))\n",
    "      image = image.resize((224,224))\n",
    "    except: \n",
    "      return None\n",
    "\n",
    "    inputs = processor(text=text, images=image, return_tensors=\"pt\", padding='max_length', max_length = 64, truncation = True)\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"].view(-1)\n",
    "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].view(-1)\n",
    "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].view(3, 224, 224)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))\n",
    "\n",
    "def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "def Trainer(train_args, model, train_dataset, val_dataset):\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size = train_args[\"batch_size\"], shuffle = True, collate_fn=collate_fn)\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size = train_args[\"batch_size\"], shuffle = True, collate_fn=collate_fn)\n",
    "  model = model.to(device)\n",
    "  optimizer = optim.AdamW(model.parameters(), lr = train_args[\"lr\"])\n",
    "  best_loss = float('inf')\n",
    "\n",
    "  train_args[\"num_train_data\"] = len(train_dataset)\n",
    "  print(f\"Training args: {train_args}\")\n",
    "\n",
    "  for epoch in range(train_args[\"epoch\"]):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        inputs = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        text_embeds = None\n",
    "        image_embeds = None\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        tmp_text_embeds = outputs.text_embeds\n",
    "        tmp_image_embeds = outputs.image_embeds\n",
    "\n",
    "        if not text_embeds:\n",
    "          text_embeds = tmp_text_embeds\n",
    "          image_embeds = tmp_image_embeds\n",
    "        else:\n",
    "          text_embeds = torch.cat((text_embeds, tmp_text_embeds), 0)\n",
    "          image_embeds = torch.cat((image_embeds, tmp_image_embeds), 0)\n",
    "\n",
    "\n",
    "        # weights update\n",
    "        if ((batch_idx + 1) % train_args[\"accum_iter\"] == 0) or (batch_idx + 1 == len(train_dataloader)):\n",
    "          # cosine similarity as logits\n",
    "          logit_scale = model.logit_scale\n",
    "          logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "          # logits_per_image = logits_per_text.t()\n",
    "\n",
    "          loss = None\n",
    "          loss = clip_loss(logits_per_text)\n",
    "\n",
    "          # backward pass\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "        inputs = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        text_embeds = None\n",
    "        image_embeds = None\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        tmp_text_embeds = outputs.text_embeds\n",
    "        tmp_image_embeds = outputs.image_embeds\n",
    "\n",
    "        if not text_embeds:\n",
    "          text_embeds = tmp_text_embeds\n",
    "          image_embeds = tmp_image_embeds\n",
    "        else:\n",
    "          text_embeds = torch.cat((text_embeds, tmp_text_embeds), 0)\n",
    "          image_embeds = torch.cat((image_embeds, tmp_image_embeds), 0)\n",
    "        \n",
    "        if ((batch_idx + 1) % train_args[\"accum_iter\"] == 0) or (batch_idx + 1 == len(train_dataloader)):\n",
    "          # cosine similarity as logits\n",
    "          logit_scale = model.logit_scale\n",
    "          logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "          # logits_per_image = logits_per_text.t()\n",
    "\n",
    "          loss = None\n",
    "          loss = clip_loss(logits_per_text)\n",
    "          eval_loss += loss\n",
    "          \n",
    "    if eval_loss < best_loss:\n",
    "      best_loss = eval_loss\n",
    "      torch.save(model, \"clip.pt\")\n",
    "      print(f\"Epoch: {epoch}, eval_loss: {eval_loss}, updating model...\")\n",
    "    else:\n",
    "      print(f\"Epoch: {epoch}, eval_loss: {eval_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  torch.manual_seed(0)\n",
    "  device = torch.device('cuda')\n",
    "  model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "  processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "  TRAIN_ROOT = \"/tmp2/cychang/tmp\"\n",
    "  images = os.listdir(TRAIN_ROOT)\n",
    "  train_images = images[:200000]\n",
    "\n",
    "  eval_images = images[200000:210000]\n",
    "\n",
    "  train_dataset = CLIPDataset(train_images, processor, TRAIN_ROOT)\n",
    "  val_dataset = CLIPDataset(eval_images, processor, TRAIN_ROOT)\n",
    "\n",
    "\n",
    "  train_args = {\n",
    "    \"epoch\": 50,\n",
    "    \"lr\": 0.0002,\n",
    "    \"accum_iter\": 2048,\n",
    "    \"batch_size\": 32,\n",
    "  }\n",
    "\n",
    "  Trainer(\n",
    "    train_args = train_args,\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    val_dataset = val_dataset\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
