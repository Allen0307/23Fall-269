{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cychang/anaconda3/envs/NLG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args: {'epoch': 50, 'lr': 0.0002, 'accum_iter': 8192, 'batch_size': 4, 'num_train_data': 200000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/50000 [00:01<7:53:45,  1.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/50000 [00:01<3:43:40,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 512])\n",
      "torch.Size([12, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/50000 [00:02<3:36:36,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 512])\n",
      "torch.Size([20, 512])\n",
      "torch.Size([24, 512])\n",
      "torch.Size([24, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/50000 [00:02<2:39:18,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 512])\n",
      "torch.Size([28, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/50000 [00:02<2:15:07,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 512])\n",
      "torch.Size([36, 512])\n",
      "torch.Size([40, 512])\n",
      "torch.Size([40, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/50000 [00:02<2:07:49,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44, 512])\n",
      "torch.Size([44, 512])\n",
      "torch.Size([48, 512])\n",
      "torch.Size([48, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/50000 [00:03<1:52:24,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 512])\n",
      "torch.Size([52, 512])\n",
      "torch.Size([56, 512])\n",
      "torch.Size([56, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/50000 [00:03<1:45:02,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 512])\n",
      "torch.Size([60, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/50000 [00:03<1:43:39,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 512])\n",
      "torch.Size([68, 512])\n",
      "torch.Size([72, 512])\n",
      "torch.Size([72, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/50000 [00:04<2:00:12,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 512])\n",
      "torch.Size([76, 512])\n",
      "torch.Size([80, 512])\n",
      "torch.Size([80, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/50000 [00:04<1:58:18,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84, 512])\n",
      "torch.Size([84, 512])\n",
      "torch.Size([88, 512])\n",
      "torch.Size([88, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/50000 [00:04<2:47:11,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([92, 512])\n",
      "torch.Size([92, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 7.93 GiB of which 15.56 MiB is free. Including non-PyTorch memory, this process has 7.91 GiB memory in use. Of the allocated memory 6.78 GiB is allocated by PyTorch, and 388.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 178\u001b[0m\n\u001b[1;32m    168\u001b[0m val_dataset \u001b[39m=\u001b[39m CLIPDataset(eval_images, processor, TRAIN_ROOT)\n\u001b[1;32m    171\u001b[0m train_args \u001b[39m=\u001b[39m {\n\u001b[1;32m    172\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m50\u001b[39m,\n\u001b[1;32m    173\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.0002\u001b[39m,\n\u001b[1;32m    174\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39maccum_iter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m8192\u001b[39m,\n\u001b[1;32m    175\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[1;32m    176\u001b[0m }\n\u001b[0;32m--> 178\u001b[0m Trainer(\n\u001b[1;32m    179\u001b[0m   train_args \u001b[39m=\u001b[39;49m train_args,\n\u001b[1;32m    180\u001b[0m   model \u001b[39m=\u001b[39;49m model,\n\u001b[1;32m    181\u001b[0m   train_dataset \u001b[39m=\u001b[39;49m train_dataset,\n\u001b[1;32m    182\u001b[0m   val_dataset \u001b[39m=\u001b[39;49m val_dataset\n\u001b[1;32m    183\u001b[0m )\n",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m, in \u001b[0;36mTrainer\u001b[0;34m(train_args, model, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_dataloader)):\n\u001b[1;32m     73\u001b[0m     inputs \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 74\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     77\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m     79\u001b[0m     tmp_text_embeds \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mtext_embeds\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLG/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 7.93 GiB of which 15.56 MiB is free. Including non-PyTorch memory, this process has 7.91 GiB memory in use. Of the allocated memory 6.78 GiB is allocated by PyTorch, and 388.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import requests\n",
    "import sys\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "\n",
    "class CLIPDataset(Dataset):\n",
    "  def __init__(self, data_path, processor, root):\n",
    "\n",
    "    self.dataset = data_path\n",
    "    self.processor = processor\n",
    "    self.root = root\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    text = self.dataset[idx].split(\".jpg\")[-2]\n",
    "\n",
    "    try:\n",
    "      image = Image.open(os.path.join(self.root, self.dataset[idx]))\n",
    "      image = image.resize((224,224))\n",
    "    except: \n",
    "      return None\n",
    "\n",
    "    inputs = processor(text=text, images=image, return_tensors=\"pt\", padding='max_length', max_length = 64, truncation = True)\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"].view(-1)\n",
    "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].view(-1)\n",
    "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].view(3, 224, 224)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))\n",
    "\n",
    "def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "def Trainer(train_args, model, train_dataset, val_dataset):\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size = train_args[\"batch_size\"], shuffle = True, collate_fn=collate_fn)\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size = train_args[\"batch_size\"], shuffle = True, collate_fn=collate_fn)\n",
    "  model = model.to(device)\n",
    "  optimizer = optim.AdamW(model.parameters(), lr = train_args[\"lr\"])\n",
    "  best_loss = float('inf')\n",
    "\n",
    "  train_args[\"num_train_data\"] = len(train_dataset)\n",
    "  print(f\"Training args: {train_args}\")\n",
    "\n",
    "  for epoch in range(train_args[\"epoch\"]):\n",
    "\n",
    "    model.train()\n",
    "    text_embeds = None\n",
    "    image_embeds = None\n",
    "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        inputs = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        tmp_text_embeds = outputs.text_embeds\n",
    "        tmp_image_embeds = outputs.image_embeds\n",
    "\n",
    "        if text_embeds == None:\n",
    "          text_embeds = tmp_text_embeds\n",
    "          image_embeds = tmp_image_embeds\n",
    "        else:\n",
    "          pass\n",
    "          text_embeds = torch.cat((text_embeds, tmp_text_embeds), 0)\n",
    "          image_embeds = torch.cat((image_embeds, tmp_image_embeds), 0)\n",
    "\n",
    "        print(text_embeds.shape)\n",
    "        print(image_embeds.shape)\n",
    "\n",
    "\n",
    "        # weights update\n",
    "        if ((batch_idx + 1) % train_args[\"accum_iter\"] == 0) or (batch_idx + 1 == len(train_dataloader)):\n",
    "          # cosine similarity as logits\n",
    "          logit_scale = model.logit_scale\n",
    "          logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "          # logits_per_image = logits_per_text.t()\n",
    "\n",
    "          loss = None\n",
    "          loss = clip_loss(logits_per_text)\n",
    "\n",
    "          # backward pass\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          text_embeds = None\n",
    "          image_embeds = None\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    text_embeds = None\n",
    "    image_embeds = None\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "        inputs = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        tmp_text_embeds = outputs.text_embeds\n",
    "        tmp_image_embeds = outputs.image_embeds\n",
    "\n",
    "        if text_embeds == None:\n",
    "          text_embeds = tmp_text_embeds\n",
    "          image_embeds = tmp_image_embeds\n",
    "        else:\n",
    "          text_embeds = torch.cat((text_embeds, tmp_text_embeds), 0)\n",
    "          image_embeds = torch.cat((image_embeds, tmp_image_embeds), 0)\n",
    "        \n",
    "        if ((batch_idx + 1) % train_args[\"accum_iter\"] == 0) or (batch_idx + 1 == len(train_dataloader)):\n",
    "          # cosine similarity as logits\n",
    "          logit_scale = model.logit_scale\n",
    "          logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "          # logits_per_image = logits_per_text.t()\n",
    "\n",
    "          loss = None\n",
    "          loss = clip_loss(logits_per_text)\n",
    "          eval_loss += loss\n",
    "          text_embeds = None\n",
    "          image_embeds = None\n",
    "\n",
    "    if eval_loss < best_loss:\n",
    "      best_loss = eval_loss\n",
    "      torch.save(model, \"clip.pt\")\n",
    "      print(f\"Epoch: {epoch}, eval_loss: {eval_loss}, updating model...\")\n",
    "    else:\n",
    "      print(f\"Epoch: {epoch}, eval_loss: {eval_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  torch.manual_seed(0)\n",
    "  device = torch.device('cuda')\n",
    "  model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "  processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "  TRAIN_ROOT = \"/tmp2/cychang/tmp\"\n",
    "  images = os.listdir(TRAIN_ROOT)\n",
    "  train_images = images[:200000]\n",
    "\n",
    "  eval_images = images[200000:210000]\n",
    "\n",
    "  train_dataset = CLIPDataset(train_images, processor, TRAIN_ROOT)\n",
    "  val_dataset = CLIPDataset(eval_images, processor, TRAIN_ROOT)\n",
    "\n",
    "\n",
    "  train_args = {\n",
    "    \"epoch\": 50,\n",
    "    \"lr\": 0.0002,\n",
    "    \"accum_iter\": 8192,\n",
    "    \"batch_size\": 4,\n",
    "  }\n",
    "\n",
    "  Trainer(\n",
    "    train_args = train_args,\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    val_dataset = val_dataset\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
