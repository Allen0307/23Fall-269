{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Specify the path to your TSV file\n",
    "train_tsv_file_path = '/home/allenfu/cyc/23Fall-269/Train_GCC-training.tsv'\n",
    "val_tsv_file_path = '/home/allenfu/cyc/23Fall-269/Validation_GCC-1.1.0-Validation.tsv'\n",
    "\n",
    "# Read the TSV file into a DataFrame\n",
    "train_df = pd.read_csv(train_tsv_file_path, delimiter='\\t', header=None)[0]\n",
    "val_df = pd.read_csv(val_tsv_file_path, delimiter='\\t', header=None)[0]\n",
    "\n",
    "def remove_spaces(sentence):\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(f' {punctuation}', punctuation)\n",
    "    return ' '.join(sentence.split())\n",
    "\n",
    "train_df = train_df.apply(remove_spaces)\n",
    "val_df = val_df.apply(remove_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allenfu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, processor, max_length=64):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sentence = self.data.iloc[idx]\n",
    "        \n",
    "        # Tokenize and encode the source sentence\n",
    "        t5_tokens = self.tokenizer.encode_plus(\n",
    "            source_sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        t5_inputs =  {\n",
    "            'input_ids': t5_tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': t5_tokens['attention_mask'].squeeze(),\n",
    "            'target_ids': t5_tokens['input_ids'].squeeze(),  # Target is the same as the input\n",
    "            'target_mask': t5_tokens['attention_mask'].squeeze(),\n",
    "            'target': source_sentence\n",
    "        }\n",
    "\n",
    "        clip_tokens = self.processor(\n",
    "            text=source_sentence, \n",
    "            images=torch.zeros((3, 224, 224)), \n",
    "            return_tensors=\"pt\", \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        clip_inputs = {\n",
    "            'input_ids': clip_tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': clip_tokens['attention_mask'].squeeze(),\n",
    "            'pixel_values': clip_tokens[\"pixel_values\"].view(3, 224, 224),\n",
    "            'target_ids': clip_tokens['input_ids'].squeeze(),  # Target is the same as the input\n",
    "            'target_mask': clip_tokens['attention_mask'].squeeze(),\n",
    "            'target': source_sentence\n",
    "        }\n",
    "\n",
    "        return clip_inputs, t5_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, CLIPModel, CLIPProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KLIPModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, class_num = 1000):\n",
    "        super(KLIPModel, self).__init__()\n",
    "\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        self.clip_to_llama = nn.Sequential(\n",
    "          torch.nn.Linear(512, 64),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(64, 4096),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "          torch.nn.Linear(512, 64),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(64, class_num),\n",
    "          torch.nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        clip_output = self.clip(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "\n",
    "        class_logits = self.classifier(clip_output.image_embeds)\n",
    "        clip_to_llama_embeds = self.clip_to_llama(clip_output.text_embeds)\n",
    "\n",
    "        similarity_loss = None\n",
    "        labels = torch.ones((input_ids.shape[0],), dtype=torch.float32, device=input_ids.device)\n",
    "        criterion = nn.CosineEmbeddingLoss(margin=0.2)\n",
    "        similarity_loss = criterion(clip_output.text_embeds, clip_output.image_embeds, labels)\n",
    "\n",
    "        return clip_output, clip_to_llama_embeds, class_logits, similarity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, bottleneck_dim=4096):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            # nn.Linear(input_dim, bottleneck_dim),\n",
    "            # nn.LayerNorm(bottleneck_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(bottleneck_dim, output_dim),\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # self.layer = nn.Linear(input_dim, output_dim)\n",
    "        # self.norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)\n",
    "\n",
    "class KLIPEval(nn.Module):\n",
    "    def __init__(self, klip_model_path, t5_model_path, device='cuda'):\n",
    "        super(KLIPEval, self).__init__()\n",
    "        self.encoder = KLIPModel()\n",
    "        self.encoder.load_state_dict(torch.load(klip_model_path))\n",
    "        self.bottleneck = Bottleneck(512, 768)\n",
    "        self.decoder = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "        self.decoder.load_state_dict(torch.load(t5_model_path))\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "        self.device = device\n",
    "\n",
    "        # Set requires_grad to False for encoder and decoder parameters\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.decoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Set requires_grad to True for dimension transform layer parameters\n",
    "        for param in self.bottleneck.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, clip_inputs, t5_inputs, train=True):\n",
    "        if train:\n",
    "            encoder_outputs = self.encoder.clip.text_model(\n",
    "                input_ids=clip_inputs[\"input_ids\"].to(self.device), \n",
    "                attention_mask=clip_inputs[\"attention_mask\"].to(self.device),\n",
    "            )\n",
    "\n",
    "            encoder_outputs['last_hidden_state'] = self.bottleneck(encoder_outputs['last_hidden_state'])\n",
    "            output = self.decoder(\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                attention_mask=t5_inputs['attention_mask'].to(self.device),\n",
    "                labels=t5_inputs['target_ids'].to(self.device)\n",
    "            )\n",
    "            return output.loss\n",
    "        else:\n",
    "            encoder_outputs = self.encoder.clip.text_model(\n",
    "                input_ids=clip_inputs[\"input_ids\"].to(self.device), \n",
    "                attention_mask=clip_inputs[\"attention_mask\"].to(self.device),\n",
    "            )\n",
    "\n",
    "            encoder_outputs['last_hidden_state'] = self.bottleneck(encoder_outputs['last_hidden_state'])\n",
    "            output = self.decoder.generate(\n",
    "                # inputs_embeds=t5_inputs_embeds,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                attention_mask=t5_inputs['attention_mask'].to(self.device),\n",
    "                decoder_input_ids=torch.tensor([[self.tokenizer.pad_token_id]] * t5_inputs['input_ids'].shape[0]).to(self.device),\n",
    "                # max_length=64,  # Set a reasonable maximum length for generated sequences\n",
    "                # num_beams=1,  # Set to 1 for greedy decoding\n",
    "                # no_repeat_ngram_size=2,  # Avoid repeating bigrams in the output\n",
    "                # early_stopping=True\n",
    "            )\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allenfu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:199: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Epoch 1/3: 100%|██████████| 25925/25925 [1:25:02<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 0.1965398128375253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation - Epoch 1:   0%|          | 0/124 [00:00<?, ?it/s]/home/allenfu/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Validation - Epoch 1: 100%|██████████| 124/124 [00:58<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation EM Score: 0.40448232323232325\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 25925/25925 [1:25:00<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Average Loss: 0.14528056094507358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation - Epoch 2: 100%|██████████| 124/124 [00:58<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation EM Score: 0.414520202020202\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 25925/25925 [1:25:05<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Average Loss: 0.14117604197767225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation - Epoch 3: 100%|██████████| 124/124 [00:58<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation EM Score: 0.42455808080808083\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Create the dataset and DataLoader\n",
    "train_dataset = Seq2SeqDataset(train_df, tokenizer, processor)\n",
    "val_dataset = Seq2SeqDataset(val_df, tokenizer, processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "klip_model_path = '/home/allenfu/269/klip_1.pt'\n",
    "# klip_model_path = '/home/allenfu/cyc/23Fall-269/klip.pt'\n",
    "t5_model_path = '/home/allenfu/cyc/23Fall-269/t5_model.pth'\n",
    "klip_model = KLIPEval(klip_model_path, t5_model_path, device).to(device)\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = optim.AdamW(klip_model.bottleneck.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "best_em_score = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    klip_model.train()\n",
    "\n",
    "    for clip_inputs, t5_inputs in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        loss = klip_model(clip_inputs, t5_inputs, train=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}')\n",
    "\n",
    "    # Optionally update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate with Exact Match (EM) on a validation set\n",
    "    klip_model.eval()\n",
    "    with torch.no_grad():\n",
    "        em_count = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for clip_inputs, t5_inputs in tqdm(val_dataloader, desc=f'Validation - Epoch {epoch + 1}'):\n",
    "            # Generate sequences\n",
    "            generated_ids = klip_model(clip_inputs, t5_inputs, train=False).detach().cpu().numpy()\n",
    "\n",
    "            # Decode token IDs to strings\n",
    "            generated_sentences = [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "            target_sentences = t5_inputs['target']\n",
    "\n",
    "            # Check for exact match\n",
    "            em_count += sum(1 for gen, target in zip(generated_sentences, target_sentences) if gen == target)\n",
    "            total_samples += len(generated_sentences)\n",
    "\n",
    "        em_score = em_count / total_samples\n",
    "        print(f'Validation EM Score: {em_score}')\n",
    "\n",
    "        # Save the model if the EM score improves\n",
    "        if em_score > best_em_score:\n",
    "            best_em_score = em_score\n",
    "            torch.save(klip_model.state_dict(), 'klip_model.pth')\n",
    "            print(\"Model saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- a woman's ad in the middle of the night ------- author: a life in photography-- in pictures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:01<00:01,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a solitary tree with a solitary tree ------- a lot of dried fruits and nuts for sale in old fashioned traditional grocery store in city on peninsula\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:02<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm a fan of the 'finger's' and i ------- today this may look familiar, but person was the guy who discovered the first flip on a bicycle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person walking on the street ------- cyclist shown in action next to competitors wearing the yellow\n",
      "Validation EM Score: 0.00390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    em_count = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for clip_inputs, t5_inputs in tqdm(val_dataloader):\n",
    "        # Generate sequences\n",
    "        generated_ids = klip_model(clip_inputs, t5_inputs, train=False).detach().cpu().numpy()\n",
    "\n",
    "        # Decode token IDs to strings\n",
    "        generated_sentences = [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "        target_sentences = t5_inputs['target']\n",
    "\n",
    "        # Check for exact match\n",
    "        em_count += sum(1 for gen, target in zip(generated_sentences, target_sentences) if gen == target)\n",
    "        total_samples += len(generated_sentences)\n",
    "        print(generated_sentences[0], '-------', target_sentences[0])\n",
    "\n",
    "    em_score = em_count / total_samples\n",
    "    print(f'Validation EM Score: {em_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = \n",
    "clip_model.load_state_dict(torch.load('clip_model.pth'))\n",
    "with torch.no_grad():\n",
    "    em_count = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for clip_inputs, t5_inputs in tqdm(val_dataloader):\n",
    "        # Generate sequences\n",
    "        generated_ids = klip_model(clip_inputs, t5_inputs, train=False).detach().cpu().numpy()\n",
    "\n",
    "        # Decode token IDs to strings\n",
    "        generated_sentences = [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "        target_sentences = t5_inputs['target']\n",
    "\n",
    "        # Check for exact match\n",
    "        em_count += sum(1 for gen, target in zip(generated_sentences, target_sentences) if gen == target)\n",
    "        total_samples += len(generated_sentences)\n",
    "        # print(generated_sentences[0], '-------', target_sentences[0])\n",
    "\n",
    "    em_score = em_count / total_samples\n",
    "    print(f'Validation EM Score: {em_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m608.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(klip_model_path)\n",
    "torch.save(a.state_dict(), 'tmp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MeanPoolingLayer(nn.Module):\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        # Apply mean pooling along the sequence dimension\n",
    "        # Use attention mask to mask out padding tokens\n",
    "        sum_embeddings = torch.sum(token_embeddings * attention_mask.unsqueeze(-1), dim=1)\n",
    "        mean_pooled = sum_embeddings / attention_mask.sum(dim=1, keepdim=True)\n",
    "        return mean_pooled\n",
    "\n",
    "# Example usage:\n",
    "embedding_dim = 768  # Assuming your token embeddings have dimension 768\n",
    "max_sequence_length = 50  # Adjust based on your actual sequence length\n",
    "batch_size = 32  # Adjust based on your batch size\n",
    "\n",
    "# Example token embeddings (replace this with your actual token embeddings)\n",
    "token_embeddings = torch.rand((batch_size, max_sequence_length, embedding_dim))\n",
    "\n",
    "# Example attention mask (1 for valid tokens, 0 for padding)\n",
    "attention_mask = torch.randint(0, 2, (batch_size, max_sequence_length))\n",
    "\n",
    "# Instantiate the MeanPoolingLayer\n",
    "mean_pooling_layer = MeanPoolingLayer()\n",
    "\n",
    "# Apply mean pooling to get a single vector representation\n",
    "mean_pooled_output = mean_pooling_layer(token_embeddings, attention_mask)\n",
    "\n",
    "# Print the shape of the mean-pooled output\n",
    "print(mean_pooled_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allenfu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:199: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "\n",
    "class CustomT5Model(nn.Module):\n",
    "    def __init__(self, model_name='t5-base'):\n",
    "        super(CustomT5Model, self).__init__()\n",
    "\n",
    "        # Load pre-trained T5 model and tokenizer\n",
    "        self.t5_encoder = T5EncoderModel.from_pretrained(model_name)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Linear transformation and layer normalization for conditioning the decoder\n",
    "        self.linear_transform = nn.Linear(self.t5_encoder.config.hidden_size, self.t5_encoder.config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(self.t5_encoder.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through the T5 encoder\n",
    "        encoder_outputs = self.t5_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract token embeddings\n",
    "        token_embeddings = encoder_outputs.last_hidden_state\n",
    "\n",
    "        # Mean pooling over token embeddings\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1)\n",
    "        mean_pooled_embeddings = torch.sum(token_embeddings * attention_mask_expanded, dim=1) / attention_mask_expanded.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Linear transformation and layer normalization for conditioning the decoder\n",
    "        conditioned_embeddings = self.layer_norm(self.linear_transform(mean_pooled_embeddings))\n",
    "\n",
    "        return conditioned_embeddings\n",
    "\n",
    "# Example usage:\n",
    "model = CustomT5Model()\n",
    "\n",
    "# Input text\n",
    "input_text = \"Your input text goes here.\"\n",
    "input_ids = model.tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Attention mask (1 for valid tokens, 0 for padding)\n",
    "attention_mask = (input_ids != model.tokenizer.pad_token_id).float()\n",
    "\n",
    "# Forward pass through the custom T5 model\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# The 'output' now contains the conditioned embeddings for the decoder\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
