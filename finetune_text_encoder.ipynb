{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Specify the path to your TSV file\n",
    "train_tsv_file_path = '/tmp/cyc/Train_GCC-training.tsv'\n",
    "val_tsv_file_path = '/tmp/cyc/Validation_GCC-1.1.0-Validation.tsv'\n",
    "\n",
    "# Read the TSV file into a DataFrame\n",
    "train_df = pd.read_csv(train_tsv_file_path, delimiter='\\t', header=None)[0]\n",
    "val_df = pd.read_csv(val_tsv_file_path, delimiter='\\t', header=None)[0]\n",
    "\n",
    "def remove_spaces(sentence):\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(f' {punctuation}', punctuation)\n",
    "    return ' '.join(sentence.split())\n",
    "\n",
    "train_df = train_df.apply(remove_spaces)\n",
    "val_df = val_df.apply(remove_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, processor, max_length=64):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sentence = self.data.iloc[idx]\n",
    "        \n",
    "        # Tokenize and encode the source sentence\n",
    "        t5_tokens = self.tokenizer.encode_plus(\n",
    "            source_sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        t5_inputs =  {\n",
    "            'input_ids': t5_tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': t5_tokens['attention_mask'].squeeze(),\n",
    "            'target_ids': t5_tokens['input_ids'].squeeze(),  # Target is the same as the input\n",
    "            'target_mask': t5_tokens['attention_mask'].squeeze(),\n",
    "            'target': source_sentence\n",
    "        }\n",
    "\n",
    "        clip_tokens = self.processor(\n",
    "            text=source_sentence, \n",
    "            images=torch.zeros((3, 224, 224)), \n",
    "            return_tensors=\"pt\", \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        clip_inputs = {\n",
    "            'input_ids': clip_tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': clip_tokens['attention_mask'].squeeze(),\n",
    "            'pixel_values': clip_tokens[\"pixel_values\"].view(3, 224, 224),\n",
    "            'target_ids': clip_tokens['input_ids'].squeeze(),  # Target is the same as the input\n",
    "            'target_mask': clip_tokens['attention_mask'].squeeze(),\n",
    "            'target': source_sentence\n",
    "        }\n",
    "\n",
    "        return t5_inputs, clip_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, CLIPModel\n",
    "from tqdm import tqdm\n",
    "        \n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.layer = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(self.layer(x))\n",
    "\n",
    "class ClipEval(nn.Module):\n",
    "    def __init__(self, t5_model_path):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.encoder = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.bottleneck = Bottleneck(512, 768)\n",
    "        self.decoder = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "        self.decoder.load_state_dict(torch.load(t5_model_path))\n",
    "\n",
    "    def forward(self, clip_inputs, t5_inputs, train=True):\n",
    "        if train:\n",
    "            pass\n",
    "        else:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "model.safetensors: 100%|██████████| 892M/892M [00:36<00:00, 24.2MB/s] \n",
      "generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 1.45MB/s]\n",
      "Epoch 1/3:   1%|          | 302/51849 [00:59<2:49:48,  5.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 37\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     39\u001b[0m average_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Average Loss: \u001b[39m\u001b[39m{\u001b[39;00maverage_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Create the dataset and DataLoader\n",
    "train_dataset = Seq2SeqDataset(train_df, tokenizer)\n",
    "val_dataset = Seq2SeqDataset(val_df, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "autoencoder_model = Autoencoder().to(device)\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = optim.AdamW(autoencoder_model.parameters(), lr=5e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    autoencoder_model.train()\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        target_mask = batch['target_mask'].to(device)\n",
    "\n",
    "        loss = autoencoder_model(input_ids, attention_mask, target_ids, target_mask)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}')\n",
    "\n",
    "    # Optionally update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate with Exact Match (EM) on a validation set\n",
    "    autoencoder_model.eval()\n",
    "    best_em_score = 0.0\n",
    "    with torch.no_grad():\n",
    "        em_count = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for val_batch in tqdm(val_dataloader, desc=f'Validation - Epoch {epoch + 1}'):\n",
    "            input_ids = val_batch['input_ids'].to(device)\n",
    "            attention_mask = val_batch['attention_mask'].to(device)\n",
    "            target_ids = val_batch['target_ids'].to(device)\n",
    "            target_mask = val_batch['target_mask'].to(device)\n",
    "\n",
    "            # Generate sequences\n",
    "            generated_ids = autoencoder_model(input_ids, attention_mask).cpu().numpy()\n",
    "\n",
    "            # Decode token IDs to strings\n",
    "            generated_sentences = [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "            target_sentences = val_batch['target']\n",
    "\n",
    "            # Check for exact match\n",
    "            em_count += sum(1 for gen, target in zip(generated_sentences, target_sentences) if gen == target)\n",
    "            total_samples += len(generated_sentences)\n",
    "\n",
    "        em_score = em_count / total_samples\n",
    "        print(f'Validation EM Score: {em_score}')\n",
    "\n",
    "        # Save the model if the EM score improves\n",
    "        if em_score > best_em_score:\n",
    "            best_em_score = em_score\n",
    "            torch.save(autoencoder_model.state_dict(), 't5_model.pth')\n",
    "            print(\"Model saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m608.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
